"""YouTubeãƒˆãƒ¬ãƒ³ãƒ‰ï¼†ç«¶åˆåˆ†æãƒ„ãƒ¼ãƒ«."""

from datetime import datetime, timedelta

import pandas as pd
import streamlit as st

from src.analyzer import (
    fetch_and_analyze,
    filter_videos,
    sort_by_vs_ratio,
    videos_to_dataframe,
)
from src.suggest_api import (
    fetch_suggestions,
    fetch_suggestions_with_alphabet_soup,
    flatten_unique_suggestions,
)
from src.trends_api import (
    get_trending_searches,
    get_interest_over_time,
    get_related_queries,
)
from src.trending import (
    CATEGORY_MAP,
    fetch_trending_videos,
    fetch_trending_all_categories,
    flatten_category_videos,
    extract_keywords_from_titles,
    analyze_trending_categories,
    trending_to_dataframe,
)
from src.utils import format_number, video_url
from src.youtube_api import (
    QuotaExceededError,
    get_quota_tracker,
    get_youtube_client,
    get_video_details,
)

# â”€â”€â”€ ãƒšãƒ¼ã‚¸è¨­å®š â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(
    page_title="YouTube ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ",
    page_icon="ğŸ”",
    layout="wide",
)

st.title("YouTube ãƒˆãƒ¬ãƒ³ãƒ‰ï¼†ç«¶åˆåˆ†æãƒ„ãƒ¼ãƒ«")

# â”€â”€â”€ APIã‚­ãƒ¼ãƒã‚§ãƒƒã‚¯ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
try:
    api_key = st.secrets["YOUTUBE_API_KEY"]
except (KeyError, FileNotFoundError):
    api_key = ""

if not api_key:
    st.error(
        "YouTube API ã‚­ãƒ¼ãŒæœªè¨­å®šã§ã™ã€‚\n\n"
        "**ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã®å ´åˆ:**\n"
        "`.streamlit/secrets.toml` ã«ä»¥ä¸‹ã‚’è¿½åŠ :\n"
        '```\nYOUTUBE_API_KEY = "YOUR_API_KEY"\n```\n\n'
        "**Streamlit Community Cloud ã®å ´åˆ:**\n"
        "ã‚¢ãƒ—ãƒªè¨­å®š â†’ Secrets ã«ä¸Šè¨˜ã¨åŒã˜å†…å®¹ã‚’å…¥åŠ›\n\n"
        "**APIã‚­ãƒ¼å–å¾—æ‰‹é †:**\n"
        "1. [Google Cloud Console](https://console.cloud.google.com/) ã§ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆä½œæˆ\n"
        "2. YouTube Data API v3 ã‚’æœ‰åŠ¹åŒ–\n"
        "3. APIã‚­ãƒ¼ã‚’ä½œæˆ"
    )
    st.stop()

# â”€â”€â”€ ã‚µã‚¤ãƒ‰ãƒãƒ¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with st.sidebar:
    st.header("æ¤œç´¢è¨­å®š")
    search_query = st.text_input("æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", value="ä¸å‹•ç”£æŠ•è³‡", placeholder="ä¾‹: ä¸å‹•ç”£æŠ•è³‡")

    st.divider()
    st.subheader("ãƒ•ã‚£ãƒ«ã‚¿")

    max_subscribers = st.number_input(
        "ç™»éŒ²è€…æ•°ä¸Šé™",
        min_value=0,
        value=0,
        step=10000,
        help="0 = åˆ¶é™ãªã—",
    )

    min_views = st.number_input(
        "å†ç”Ÿæ•°ä¸‹é™",
        min_value=0,
        value=0,
        step=1000,
        help="0 = åˆ¶é™ãªã—",
    )

    min_vs_ratio = st.number_input(
        "V/Sæ¯”ç‡ä¸‹é™",
        min_value=0.0,
        value=0.0,
        step=0.1,
        format="%.1f",
        help="0.0 = åˆ¶é™ãªã—",
    )

    period_options = {
        "åˆ¶é™ãªã—": None,
        "éå»7æ—¥": 7,
        "éå»30æ—¥": 30,
        "éå»90æ—¥": 90,
        "éå»1å¹´": 365,
    }
    period_label = st.selectbox("æœŸé–“", options=list(period_options.keys()))
    period_days = period_options[period_label]

    st.divider()
    st.subheader("ã‚¯ã‚©ãƒ¼ã‚¿çŠ¶æ³")
    tracker = get_quota_tracker()
    st.metric("ä½¿ç”¨é‡", f"{tracker.used:,} / {tracker.daily_limit:,}")
    st.progress(min(tracker.usage_percent / 100, 1.0))
    st.caption(f"æ®‹ã‚Šç´„ {tracker.remaining:,} ãƒ¦ãƒ‹ãƒƒãƒˆ")

# â”€â”€â”€ ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ï¼ˆã‚¿ãƒ–ï¼‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
tab_hot, tab_genre, tab_suggest, tab_buzz, tab_trends = st.tabs(["æ€¥ä¸Šæ˜‡ãƒˆãƒ¬ãƒ³ãƒ‰", "ã‚¸ãƒ£ãƒ³ãƒ«åˆ¥ãƒ©ãƒ³ã‚­ãƒ³ã‚°", "ã‚µã‚¸ã‚§ã‚¹ãƒˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", "ãƒã‚ºå‹•ç”»åˆ†æ", "ãƒˆãƒ¬ãƒ³ãƒ‰èª¿æŸ»"])

# â”€â”€â”€ ã‚¿ãƒ–0: æ€¥ä¸Šæ˜‡ãƒˆãƒ¬ãƒ³ãƒ‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with tab_hot:
    st.subheader("YouTube æ€¥ä¸Šæ˜‡ãƒˆãƒ¬ãƒ³ãƒ‰ï¼ˆæ—¥æœ¬ï¼‰")
    st.caption("ä»ŠYouTubeæ—¥æœ¬ã§ä½•ãŒãƒã‚ºã£ã¦ã„ã‚‹ã‹ã‚’ä¸€ç›®ã§æŠŠæ¡ã§ãã¾ã™ã€‚ã‚¯ã‚©ãƒ¼ã‚¿æ¶ˆè²»: 1ãƒ¦ãƒ‹ãƒƒãƒˆ/å›")

    if st.button("æ€¥ä¸Šæ˜‡ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆå…¨ã‚«ãƒ†ã‚´ãƒªï¼‰", type="primary", use_container_width=True):
        try:
            with st.spinner("å…¨ã‚«ãƒ†ã‚´ãƒªã®æ€¥ä¸Šæ˜‡å‹•ç”»ã‚’å–å¾—ä¸­ï¼ˆç´„15ãƒ¦ãƒ‹ãƒƒãƒˆæ¶ˆè²»ï¼‰..."):
                category_videos = fetch_trending_all_categories(api_key)
                all_videos = flatten_category_videos(category_videos)

            st.session_state["trending_videos"] = all_videos
            st.session_state["trending_by_category"] = category_videos
        except QuotaExceededError:
            st.warning("APIã‚¯ã‚©ãƒ¼ã‚¿ã‚’è¶…éã—ã¾ã—ãŸã€‚")

    if "trending_videos" in st.session_state and st.session_state["trending_videos"]:
        trending_videos = st.session_state["trending_videos"]
        st.success(f"{len(trending_videos)} ä»¶ã®æ€¥ä¸Šæ˜‡å‹•ç”»ã‚’å–å¾—ã—ã¾ã—ãŸ")

        # ãƒã‚ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ©ãƒ³ã‚­ãƒ³ã‚°
        st.markdown("### ãƒã‚ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ TOP30")
        st.caption("æ€¥ä¸Šæ˜‡å‹•ç”»ã®ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰é »å‡ºãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º")
        keywords = extract_keywords_from_titles(trending_videos)
        if keywords:
            kw_df = pd.DataFrame(keywords, columns=["ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", "å‡ºç¾å›æ•°"])
            col_chart, col_table = st.columns([2, 1])
            with col_chart:
                st.bar_chart(kw_df.set_index("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰"), horizontal=True, height=600)
            with col_table:
                st.dataframe(kw_df, use_container_width=True, height=600)

        # ã‚«ãƒ†ã‚´ãƒªåˆ†å¸ƒ
        st.markdown("### ã‚«ãƒ†ã‚´ãƒªåˆ†å¸ƒ")
        cat_df = analyze_trending_categories(trending_videos)
        if not cat_df.empty:
            col_pie, col_cat_table = st.columns([2, 1])
            with col_pie:
                st.bar_chart(cat_df.set_index("ã‚«ãƒ†ã‚´ãƒª"), height=400)
            with col_cat_table:
                st.dataframe(cat_df, use_container_width=True)

        # æ€¥ä¸Šæ˜‡å‹•ç”»ã‚µãƒ ãƒã‚¤ãƒ«ä¸€è¦§
        st.markdown("### æ€¥ä¸Šæ˜‡å‹•ç”»ä¸€è¦§")
        cols_per_row = 3
        for i in range(0, min(len(trending_videos), 15), cols_per_row):
            cols = st.columns(cols_per_row)
            for j, col in enumerate(cols):
                idx = i + j
                if idx >= len(trending_videos):
                    break
                v = trending_videos[idx]
                snippet = v.get("snippet", {})
                stats = v.get("statistics", {})
                thumbnails = snippet.get("thumbnails", {})
                thumb_url = (
                    thumbnails.get("high", {}).get("url")
                    or thumbnails.get("medium", {}).get("url")
                    or thumbnails.get("default", {}).get("url", "")
                )
                with col:
                    with st.container(border=True):
                        st.image(thumb_url, use_container_width=True)
                        vid_url = f"https://www.youtube.com/watch?v={v['id']}"
                        st.markdown(f"**[{snippet.get('title', '')}]({vid_url})**")
                        st.caption(f"{snippet.get('channelTitle', '')} / {format_number(int(stats.get('viewCount', 0)))}å›å†ç”Ÿ")

        # ãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«ï¼†CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        st.divider()
        df_trending = trending_to_dataframe(trending_videos)
        st.dataframe(df_trending, use_container_width=True, height=400)

        csv_trending = df_trending.to_csv(index=False).encode("utf-8-sig")
        st.download_button(
            "CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            csv_trending,
            file_name="youtube_trending_jp.csv",
            mime="text/csv",
            key="trending_csv",
        )

# â”€â”€â”€ ã‚¿ãƒ–: ã‚¸ãƒ£ãƒ³ãƒ«åˆ¥ãƒ©ãƒ³ã‚­ãƒ³ã‚° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with tab_genre:
    st.subheader("ã‚¸ãƒ£ãƒ³ãƒ«åˆ¥ äººæ°—ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ©ãƒ³ã‚­ãƒ³ã‚°")
    st.caption("æŒ‡å®šã‚¸ãƒ£ãƒ³ãƒ«ãƒ»æœŸé–“ã§å†ç”Ÿæ•°ã®å¤šã„å‹•ç”»ã‹ã‚‰ãƒã‚ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºã—ã¾ã™ã€‚ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢: 102ãƒ¦ãƒ‹ãƒƒãƒˆ / ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãªã—: 1ãƒ¦ãƒ‹ãƒƒãƒˆ")

    _genre_map = {"å…¨ã‚¸ãƒ£ãƒ³ãƒ«": "0"}
    _genre_map.update({v: k for k, v in CATEGORY_MAP.items()})

    genre_col1, genre_col2 = st.columns(2)
    with genre_col1:
        selected_genre = st.selectbox(
            "ã‚¸ãƒ£ãƒ³ãƒ«", options=list(_genre_map.keys()), key="genre_select",
        )
    with genre_col2:
        genre_period_options = {
            "éå»7æ—¥": 7,
            "éå»30æ—¥": 30,
            "éå»90æ—¥": 90,
            "éå»1å¹´": 365,
        }
        selected_period = st.selectbox(
            "æœŸé–“", options=list(genre_period_options.keys()), key="genre_period",
        )

    genre_keyword = st.text_input(
        "æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆä»»æ„ï¼‰",
        value=search_query,
        key="genre_keyword",
        help="ç©ºæ¬„ã®å ´åˆã¯ã‚¸ãƒ£ãƒ³ãƒ«å…¨ä½“ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚’å–å¾—ã—ã¾ã™",
    )

    if st.button("ãƒ©ãƒ³ã‚­ãƒ³ã‚°å–å¾—", type="primary", use_container_width=True, key="genre_btn"):
        days = genre_period_options[selected_period]
        category_id = _genre_map[selected_genre]
        has_keyword = bool(genre_keyword.strip())

        try:
            if has_keyword:
                # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚ã‚Š â†’ search.listï¼ˆ100ãƒ¦ãƒ‹ãƒƒãƒˆï¼‰
                with st.spinner(f"ã€Œ{genre_keyword.strip()}ã€Ã— {selected_genre} ã‚’æ¤œç´¢ä¸­..."):
                    dt = datetime.utcnow() - timedelta(days=days)
                    published_after = dt.strftime("%Y-%m-%dT%H:%M:%SZ")
                    youtube = get_youtube_client(api_key)

                    params = {
                        "part": "snippet",
                        "type": "video",
                        "q": genre_keyword.strip(),
                        "maxResults": 50,
                        "order": "viewCount",
                        "regionCode": "JP",
                        "publishedAfter": published_after,
                    }
                    if category_id != "0":
                        params["videoCategoryId"] = category_id

                    response = youtube.search().list(**params).execute()
                    tracker.add(100)
                    items = response.get("items", [])

                    # video details ã§å†ç”Ÿæ•°å–å¾—
                    video_ids = tuple(
                        item["id"]["videoId"]
                        for item in items
                        if item.get("id", {}).get("videoId")
                    )
                    video_stats = get_video_details(api_key, video_ids) if video_ids else {}

                    genre_videos = []
                    for item in items:
                        vid = item.get("id", {}).get("videoId", "")
                        if not vid:
                            continue
                        snippet = item["snippet"]
                        stats = video_stats.get(vid, {})
                        thumbnails = snippet.get("thumbnails", {})
                        thumb_url = (
                            thumbnails.get("high", {}).get("url")
                            or thumbnails.get("medium", {}).get("url")
                            or thumbnails.get("default", {}).get("url", "")
                        )
                        genre_videos.append({
                            "id": vid,
                            "snippet": snippet,
                            "statistics": stats,
                            "thumbnail_url": thumb_url,
                            "view_count": int(stats.get("viewCount", 0)),
                        })
            else:
                # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãªã— â†’ videos.list mostPopularï¼ˆ1ãƒ¦ãƒ‹ãƒƒãƒˆ/ã‚«ãƒ†ã‚´ãƒªï¼‰
                with st.spinner(f"ã€Œ{selected_genre}ã€ã®äººæ°—å‹•ç”»ã‚’å–å¾—ä¸­..."):
                    if category_id == "0":
                        # å…¨ã‚¸ãƒ£ãƒ³ãƒ«: ã‚«ãƒ†ã‚´ãƒªã”ã¨ã«å–å¾—
                        all_items = []
                        for cat_id in CATEGORY_MAP:
                            vids = fetch_trending_videos(
                                api_key, category_id=cat_id, max_results=50,
                            )
                            all_items.extend(vids)
                    else:
                        all_items = fetch_trending_videos(
                            api_key, category_id=category_id, max_results=50,
                        )

                    genre_videos = []
                    seen_ids = set()
                    for v in all_items:
                        vid = v.get("id", "")
                        if vid in seen_ids:
                            continue
                        seen_ids.add(vid)
                        snippet = v.get("snippet", {})
                        stats = v.get("statistics", {})
                        thumbnails = snippet.get("thumbnails", {})
                        thumb_url = (
                            thumbnails.get("high", {}).get("url")
                            or thumbnails.get("medium", {}).get("url")
                            or thumbnails.get("default", {}).get("url", "")
                        )
                        genre_videos.append({
                            "id": vid,
                            "snippet": snippet,
                            "statistics": stats,
                            "thumbnail_url": thumb_url,
                            "view_count": int(stats.get("viewCount", 0)),
                        })

            genre_videos.sort(key=lambda x: x["view_count"], reverse=True)
            st.session_state["genre_videos"] = genre_videos
            st.session_state["genre_label"] = f"{selected_genre}ï¼ˆ{selected_period}ï¼‰"

            if not genre_videos:
                st.warning("è©²å½“ã™ã‚‹å‹•ç”»ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚æ¡ä»¶ã‚’å¤‰ãˆã¦ãŠè©¦ã—ãã ã•ã„ã€‚")

        except QuotaExceededError:
            st.warning("APIã‚¯ã‚©ãƒ¼ã‚¿ã‚’è¶…éã—ã¾ã—ãŸã€‚")
        except Exception as e:
            st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {type(e).__name__}: {e}")

    if "genre_videos" in st.session_state and st.session_state["genre_videos"]:
        genre_videos = st.session_state["genre_videos"]
        label = st.session_state.get("genre_label", "")
        st.success(f"ã€Œ{label}ã€: {len(genre_videos)} ä»¶ã®å‹•ç”»ã‚’å–å¾—")

        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ©ãƒ³ã‚­ãƒ³ã‚°
        st.markdown(f"### ãƒã‚ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ ãƒ©ãƒ³ã‚­ãƒ³ã‚° - {label}")
        # snippetæ§‹é€ ã‚’æŒã¤ã®ã§ãã®ã¾ã¾ä½¿ãˆã‚‹
        keywords = extract_keywords_from_titles(genre_videos)
        if keywords:
            kw_df = pd.DataFrame(keywords, columns=["ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", "å‡ºç¾å›æ•°"])
            col_chart, col_table = st.columns([2, 1])
            with col_chart:
                st.bar_chart(kw_df.set_index("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰"), horizontal=True, height=600)
            with col_table:
                st.dataframe(kw_df, use_container_width=True, height=600)

        # å‹•ç”»ã‚µãƒ ãƒã‚¤ãƒ«ä¸€è¦§
        st.markdown(f"### äººæ°—å‹•ç”» TOP15 - {label}")
        cols_per_row = 3
        for i in range(0, min(len(genre_videos), 15), cols_per_row):
            cols = st.columns(cols_per_row)
            for j, col in enumerate(cols):
                idx = i + j
                if idx >= len(genre_videos):
                    break
                v = genre_videos[idx]
                snippet = v.get("snippet", {})
                with col:
                    with st.container(border=True):
                        st.image(v["thumbnail_url"], use_container_width=True)
                        vid_url = f"https://www.youtube.com/watch?v={v['id']}"
                        st.markdown(f"**[{snippet.get('title', '')}]({vid_url})**")
                        st.caption(
                            f"{snippet.get('channelTitle', '')} / "
                            f"{format_number(v['view_count'])}å›å†ç”Ÿ"
                        )

        # CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        st.divider()
        rows = []
        for v in genre_videos:
            snippet = v.get("snippet", {})
            rows.append({
                "ã‚¿ã‚¤ãƒˆãƒ«": snippet.get("title", ""),
                "ãƒãƒ£ãƒ³ãƒãƒ«": snippet.get("channelTitle", ""),
                "å†ç”Ÿæ•°": v["view_count"],
                "å…¬é–‹æ—¥": snippet.get("publishedAt", "")[:10],
                "å‹•ç”»URL": f"https://www.youtube.com/watch?v={v['id']}",
            })
        df_genre = pd.DataFrame(rows)
        st.dataframe(df_genre, use_container_width=True, height=400)

        csv_genre = df_genre.to_csv(index=False).encode("utf-8-sig")
        st.download_button(
            "CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            csv_genre,
            file_name=f"genre_ranking_{selected_genre}.csv",
            mime="text/csv",
            key="genre_csv",
        )

# â”€â”€â”€ ã‚¿ãƒ–1: ã‚µã‚¸ã‚§ã‚¹ãƒˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with tab_suggest:
    st.subheader("ã‚µã‚¸ã‚§ã‚¹ãƒˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åé›†")
    st.caption("Googleã‚µã‚¸ã‚§ã‚¹ãƒˆéå…¬å¼APIã‚’ä½¿ã£ã¦ã€é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ç¶²ç¾…çš„ã«å–å¾—ã—ã¾ã™ã€‚")

    col_base, col_soup = st.columns(2)

    with col_base:
        if st.button("åŸºæœ¬ã‚µã‚¸ã‚§ã‚¹ãƒˆå–å¾—", use_container_width=True):
            if not search_query:
                st.warning("æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
            else:
                with st.spinner("ã‚µã‚¸ã‚§ã‚¹ãƒˆå–å¾—ä¸­..."):
                    suggestions = fetch_suggestions(search_query)
                st.session_state["base_suggestions"] = suggestions

    with col_soup:
        if st.button("ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆã‚¹ãƒ¼ãƒ—å–å¾—ï¼ˆç´„2åˆ†ï¼‰", use_container_width=True):
            if not search_query:
                st.warning("æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
            else:
                progress_bar = st.progress(0, text="ã‚µã‚¸ã‚§ã‚¹ãƒˆåé›†ä¸­...")

                def update_progress(current: int, total: int):
                    progress_bar.progress(
                        current / total,
                        text=f"ã‚µã‚¸ã‚§ã‚¹ãƒˆåé›†ä¸­... ({current}/{total})",
                    )

                soup_results = fetch_suggestions_with_alphabet_soup(
                    search_query,
                    delay=1.5,
                    progress_callback=update_progress,
                )
                progress_bar.empty()

                base = st.session_state.get("base_suggestions", [])
                if not base:
                    base = fetch_suggestions(search_query)
                    st.session_state["base_suggestions"] = base

                all_keywords = flatten_unique_suggestions(base, soup_results)
                st.session_state["all_suggestions"] = all_keywords

    # çµæœè¡¨ç¤º
    if "all_suggestions" in st.session_state and st.session_state["all_suggestions"]:
        keywords = st.session_state["all_suggestions"]
        st.success(f"{len(keywords)} ä»¶ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å–å¾—ã—ã¾ã—ãŸ")

        df_kw = pd.DataFrame({"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰": keywords})
        st.dataframe(df_kw, use_container_width=True, height=400)

        csv_kw = df_kw.to_csv(index=False).encode("utf-8-sig")
        st.download_button(
            "CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            csv_kw,
            file_name=f"suggestions_{search_query}.csv",
            mime="text/csv",
        )

    elif "base_suggestions" in st.session_state and st.session_state["base_suggestions"]:
        suggestions = st.session_state["base_suggestions"]
        st.info(f"{len(suggestions)} ä»¶ã®åŸºæœ¬ã‚µã‚¸ã‚§ã‚¹ãƒˆã‚’å–å¾—ã—ã¾ã—ãŸ")

        df_base = pd.DataFrame({"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰": suggestions})
        st.dataframe(df_base, use_container_width=True)

# â”€â”€â”€ ã‚¿ãƒ–2: ãƒã‚ºå‹•ç”»åˆ†æ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with tab_buzz:
    st.subheader("ãƒã‚ºå‹•ç”»åˆ†æï¼ˆV/Sæ¯”ç‡ï¼‰")
    st.caption("V/Sæ¯”ç‡ = å†ç”Ÿæ•° / ç™»éŒ²è€…æ•°ã€‚é«˜ã„ã»ã©ä¼ç”»åŠ›ã®ã‚ã‚‹å‹•ç”»ã§ã™ã€‚")

    if st.button("åˆ†æé–‹å§‹", type="primary", use_container_width=True):
        if not search_query:
            st.warning("ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        else:
            published_after = None
            if period_days:
                dt = datetime.utcnow() - timedelta(days=period_days)
                published_after = dt.strftime("%Y-%m-%dT%H:%M:%SZ")

            try:
                with st.spinner("YouTube APIã‹ã‚‰å‹•ç”»ã‚’æ¤œç´¢ä¸­..."):
                    videos = fetch_and_analyze(
                        api_key, search_query, published_after=published_after
                    )

                # ãƒ•ã‚£ãƒ«ã‚¿é©ç”¨
                filtered = filter_videos(
                    videos,
                    max_subscribers=max_subscribers if max_subscribers > 0 else None,
                    min_views=min_views if min_views > 0 else None,
                    min_vs_ratio=min_vs_ratio if min_vs_ratio > 0 else None,
                )

                # V/Sæ¯”ç‡ã§ã‚½ãƒ¼ãƒˆ
                sorted_videos = sort_by_vs_ratio(filtered)
                st.session_state["analyzed_videos"] = sorted_videos

            except QuotaExceededError:
                st.warning(
                    "APIã‚¯ã‚©ãƒ¼ã‚¿ã‚’è¶…éã—ã¾ã—ãŸã€‚"
                    "ã‚¯ã‚©ãƒ¼ã‚¿ã¯å¤ªå¹³æ´‹æ™‚é–“ã®åˆå‰0æ™‚ï¼ˆæ—¥æœ¬æ™‚é–“16:00ï¼‰ã«ãƒªã‚»ãƒƒãƒˆã•ã‚Œã¾ã™ã€‚"
                )

    # çµæœè¡¨ç¤º
    if "analyzed_videos" in st.session_state and st.session_state["analyzed_videos"]:
        sorted_videos = st.session_state["analyzed_videos"]
        st.success(f"{len(sorted_videos)} ä»¶ã®å‹•ç”»ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")

        # ã‚µãƒ ãƒã‚¤ãƒ«ã‚°ãƒªãƒƒãƒ‰è¡¨ç¤º
        cols_per_row = 3
        for i in range(0, len(sorted_videos), cols_per_row):
            cols = st.columns(cols_per_row)
            for j, col in enumerate(cols):
                idx = i + j
                if idx >= len(sorted_videos):
                    break
                v = sorted_videos[idx]
                with col:
                    with st.container(border=True):
                        st.image(v.thumbnail_url, use_container_width=True)
                        st.markdown(
                            f"**[{v.title}]({video_url(v.video_id)})**"
                        )
                        metric_cols = st.columns(3)
                        metric_cols[0].metric("V/Sæ¯”ç‡", f"{v.vs_ratio:.1f}")
                        metric_cols[1].metric("å†ç”Ÿæ•°", format_number(v.view_count))
                        metric_cols[2].metric("ç™»éŒ²è€…", format_number(v.subscriber_count))

        # ãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«ï¼†CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        st.divider()
        df = videos_to_dataframe(sorted_videos)
        st.dataframe(df, use_container_width=True, height=400)

        csv = df.to_csv(index=False).encode("utf-8-sig")
        st.download_button(
            "CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            csv,
            file_name=f"buzz_videos_{search_query}.csv",
            mime="text/csv",
        )

    elif "analyzed_videos" in st.session_state:
        st.info("æ¡ä»¶ã«ä¸€è‡´ã™ã‚‹å‹•ç”»ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ•ã‚£ãƒ«ã‚¿æ¡ä»¶ã‚’ç·©ã‚ã¦ãŠè©¦ã—ãã ã•ã„ã€‚")

# â”€â”€â”€ ã‚¿ãƒ–3: ãƒˆãƒ¬ãƒ³ãƒ‰èª¿æŸ» â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with tab_trends:
    st.subheader("ãƒˆãƒ¬ãƒ³ãƒ‰èª¿æŸ»")
    st.caption("Google Trends ã®ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã€ä»Šæ³¨ç›®ã•ã‚Œã¦ã„ã‚‹æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ç¢ºèªã§ãã¾ã™ã€‚APIã‚¯ã‚©ãƒ¼ã‚¿æ¶ˆè²»ãªã—ã€‚")

    # â”€â”€ ã‚»ã‚¯ã‚·ãƒ§ãƒ³1: ä»Šæ—¥ã®æ€¥ä¸Šæ˜‡ãƒ¯ãƒ¼ãƒ‰ï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä¸è¦ï¼‰ â”€â”€
    st.markdown("### æ€¥ä¸Šæ˜‡æ¤œç´¢ãƒ¯ãƒ¼ãƒ‰ï¼ˆæ—¥æœ¬ãƒ»éå»7æ—¥é–“ï¼‰")
    st.caption("Googleæ¤œç´¢ã§ç›´è¿‘7æ—¥é–“ã«æ€¥ä¸Šæ˜‡ã—ãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä¸€è¦§ã€‚ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŒ‡å®šä¸è¦ã§å–å¾—ã§ãã¾ã™ã€‚")

    if st.button("æ€¥ä¸Šæ˜‡ãƒ¯ãƒ¼ãƒ‰ã‚’å–å¾—", use_container_width=True, key="trending_searches_btn"):
        try:
            with st.spinner("Google Trends æ€¥ä¸Šæ˜‡ãƒ¯ãƒ¼ãƒ‰ã‚’å–å¾—ä¸­..."):
                trending_df = get_trending_searches(geo="JP")
            st.session_state["trending_searches"] = trending_df
        except Exception as e:
            st.error(f"å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

    if "trending_searches" in st.session_state:
        trending_df = st.session_state["trending_searches"]
        if not trending_df.empty:
            st.success(f"{len(trending_df)} ä»¶ã®æ€¥ä¸Šæ˜‡ãƒ¯ãƒ¼ãƒ‰ã‚’å–å¾—")
            st.dataframe(trending_df, use_container_width=True, height=500)

            csv_trending_kw = trending_df.to_csv().encode("utf-8-sig")
            st.download_button(
                "CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                csv_trending_kw,
                file_name="google_trending_searches_jp.csv",
                mime="text/csv",
                key="trending_searches_csv",
            )
        else:
            st.info("æ€¥ä¸Šæ˜‡ãƒ¯ãƒ¼ãƒ‰ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")

    # â”€â”€ ã‚»ã‚¯ã‚·ãƒ§ãƒ³2: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ¥ã®æ¤œç´¢ãƒœãƒªãƒ¥ãƒ¼ãƒ èª¿æŸ» â”€â”€
    st.divider()
    st.markdown("### ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ¥ æ¤œç´¢ãƒœãƒªãƒ¥ãƒ¼ãƒ èª¿æŸ»")
    st.caption("ç‰¹å®šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®æ¤œç´¢ãƒœãƒªãƒ¥ãƒ¼ãƒ æ¨ç§»ã¨é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚")

    trend_period = st.selectbox(
        "èª¿æŸ»æœŸé–“",
        options=["éå»12ãƒ¶æœˆ", "éå»3ãƒ¶æœˆ", "éå»1ãƒ¶æœˆ", "éå»7æ—¥"],
        key="trend_period",
    )
    period_map = {
        "éå»12ãƒ¶æœˆ": "today 12-m",
        "éå»3ãƒ¶æœˆ": "today 3-m",
        "éå»1ãƒ¶æœˆ": "today 1-m",
        "éå»7æ—¥": "now 7-d",
    }

    if st.button("ãƒˆãƒ¬ãƒ³ãƒ‰èª¿æŸ»é–‹å§‹", type="primary", use_container_width=True):
        if not search_query:
            st.warning("ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        else:
            try:
                with st.spinner("Google Trends ãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­..."):
                    timeframe = period_map[trend_period]
                    interest_df = get_interest_over_time(search_query, timeframe=timeframe)
                    related = get_related_queries(search_query)

                st.session_state["trend_interest"] = interest_df
                st.session_state["trend_related"] = related
                st.session_state["trend_keyword"] = search_query

            except Exception as e:
                st.error(f"ãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

    if "trend_interest" in st.session_state and st.session_state.get("trend_keyword"):
        kw = st.session_state["trend_keyword"]
        interest_df = st.session_state["trend_interest"]
        related = st.session_state["trend_related"]

        # æ¤œç´¢ãƒœãƒªãƒ¥ãƒ¼ãƒ æ¨ç§»ã‚°ãƒ©ãƒ•
        st.markdown(f"### ã€Œ{kw}ã€ã®æ¤œç´¢ãƒœãƒªãƒ¥ãƒ¼ãƒ æ¨ç§»")
        if not interest_df.empty:
            st.line_chart(interest_df[kw], use_container_width=True, height=300)
        else:
            st.info("ã“ã®æœŸé–“ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")

        # é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        col_rising, col_top = st.columns(2)

        with col_rising:
            st.markdown("### æ€¥ä¸Šæ˜‡ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰")
            rising_df = related.get("rising", pd.DataFrame())
            if not rising_df.empty:
                st.dataframe(rising_df, use_container_width=True, height=400)
            else:
                st.info("æ€¥ä¸Šæ˜‡ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

        with col_top:
            st.markdown("### äººæ°—ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰")
            top_df = related.get("top", pd.DataFrame())
            if not top_df.empty:
                st.dataframe(top_df, use_container_width=True, height=400)
            else:
                st.info("äººæ°—ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
